# Data Hazard Labels
 The most basic element in conducting research is data in its many forms. The methods for collecting and presenting of this data can vary greatly depending on the source and methodology which in turn can lead to problems in evaluation and conclusions. Especially the use of increasingly complex digital technologies has muddied the question of responsibility for certain outputs or decisions made by algorithms. These effects and outcomes can have consequences ranging from slight deviations in results to actively harmful effects for people and the environment. Thus a reflected and responsible approach to using data science is more important than ever, but since data can be abstract and nuanced this can be difficult.\\To give perspective on the difficulties and problems for specific works or sources using data science the Data Hazard Project was founded. Data Hazard Labels are similar to their real world counterparts and aim to help highlight aspects that could limit the outcome or lead to problems if they are not considered carefully.  These labels are not simple true or false problems and more than a single label can be applied to a dataset or a project. Each label comes with its own set of challenges and solutions and none of these labels are strict guidelines and rules, but are meant to facilitate communication and create understanding about the problems that can happen through improper use of data science.

## Generic Data Hazard
Definition

The General Data Hazard label is the most basic label that has the broadest application. All software, algorithms and data outputs using data science fall into this category. The label is meant to create accountability by giving the creators the full responsibility of the output. It also encourages vetting the data sources and giving an extensive write-up of expectations and assumptions while addressing potential shortcomings.

## Reinforces Existing Biases
This label indicates that the data, algorithm or software could result in unfair treatment of individuals and certain groups of people. There can be many reasons where this hazard could originate: \\
A bias may be exacerbated through input data, if the input data contains biases itself which are then not addressed. The design of an algorithm may also cause a bias, if it weighs certain characteristics that favour individuals or groups over others. Societal biases find their way into data and algorithms, by reflecting already existing biases which may then result in reinforced stereotypes. \\ 
These biases are often  unwanted and unintentional, but may have negative unforeseen consequences if they remain unaddressed.


## Ranks or classifies people



## Automates decision making


## Danger of misuse


## May cause direct harm

